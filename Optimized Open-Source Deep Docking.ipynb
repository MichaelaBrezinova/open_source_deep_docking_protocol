{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073ff32f",
   "metadata": {},
   "source": [
    "# Original Paper summary - *Deep Docking: A Deep Learning Platform for Augmentation of Structure Based Drug Discovery*\n",
    "- recent surge of\n",
    "small molecules availability presents great drug discovery\n",
    "opportunities, but also demands much faster screening protocols\n",
    "- there is still a global lack of experience in screening such libraries, and the advantage of docking them versus smaller collections is still matter of debate.6 However, few recently published works seem to advocate for expanding VS to ultralarge chemical libraries. \n",
    "- the current chemical space remains largely inaccessible to structure-based drug discovery - current approach to address this disparity is to filter large chemical collections to manageable drug-, lead-, fragment-, and hit-like subsets (among others) using precomputed physicochemical parameters and drug-like criteria, such as molecular weight, volume, octanol−water partition coefficient, polar surface area, number or rotatable bonds, number of hydrogen bond donors and acceptors, among many others.\n",
    "- conventional docking workflow is remarkably neglectful of negative results and the vast majority of docking data (both favorable and, especially unfavorable) is not being utilized in any way or form, while it could represent a very relevant, well-formatted, and content-rich input for machine learning algorithms\n",
    "\n",
    "## Results\n",
    "- DD achieves up to 100-fold reduction of an ultralarge docking database and up to 6000-fold enrichment for the top-ranked hits, while avoiding significant loss of favorable virtual hits, as it will be discussed below.\n",
    "- <font color=\"red\">DD pipeline: </font>\n",
    "    - For each entry of an ultralarge docking database (such as ZINC15), the standard set of ligand-based QSAR descriptors (such as molecular fingerprints) is computed;\n",
    "    - A reasonably sized training subset is randomly sampled from the database and docked into the target of interest using conventional docking protocol(s)\n",
    "    - The generated docking scores of the training compounds are then related to their 2D molecular descriptors through a DL model; a docking score cutoff (typically negative) is then used to divide training compounds in virtual hits (scoring below the cutoff) and nonhits (scoring above the cutoff) - deciding cutoff also gradually becomes more stringent over iterations\n",
    "    - The resulting QSAR deep model (trained on empirical docking scores) is then used to predict docking outcomes of yet unprocessed entries of the database. A predefined number of predicted virtual hits are then randomly sampled and used for the training set augmentation;\n",
    "    - Steps b−d are repeated iteratively until a predefined number of iterations is reached, and/or processed entries of an ultralarge docking database are converged\n",
    "- <font color=\"green\">Ultra Large Docking Database Sampling: </font>\n",
    "    - Selection of a representative and balanced training set is a critical step of any modeling workflow - a proper DD training set should effectively reflect database’s chemical diversity\n",
    "    - Biasing sampling toward molecules that are highly ranked by DD as potential virtual hits could exclude low ranked, yet true positive molecules from being selected for model training; therefore we selected random sampling for all DD iterations.\n",
    "    - To establish an optimal sampling of ZINC15 base, the relationship between the size of DD training set and the corresponding means and standard deviations of the test set recall values was evaluated\n",
    "- Size reduction by virtual screening:\n",
    "    - The main goal of DD methodology is to reduce an ultralarge docking database of billions of entries to a manageable few- million-molecules subset which yet encompasses the vast majority of virtual hits\n",
    "    - This final molecular subset can then be normally docked into the target using one or several docking programs or can be postprocessed with other VS means.\n",
    "    - DD itself is not a docking engine, but a DL score predictor to be used in conjunction with any docking program to rapidly eliminate a priori unfavorable, “undockable” molecular entities, and therefore drastically increase the speed of actual docking.\n",
    "    - The majority of nonhits were removed during the first iteration for all targets, while fewer molecules were discarded in successive steps, as expected due to larger portions of unfavorable compounds being present at the beginning of the runs. It was observed that the decrease rate and the number of hits identified were target-dependent\n",
    "- Analysis of DD performance\n",
    "    - all underlying DL models were generalizable in a consistent way\n",
    "    - there were FDRE scores comparisons done and enrichment values evaluated for top 10, 100 and 1000.\n",
    "    - true hits are highly concentrated at the top of the DD rank\n",
    "    - Overall, the above analysis indicates that the DD procedure can effectively discard most of unqualified molecules in a ultra large docking database, without losing more than a predefined percentage of virtual hits. In our opinion, this makes DD methodology an efficient mean for conducting large-scale VS campaigns involving billions of small molecule structures, and a valid alternative to brute force approaches demanding large amounts of computational resources.\n",
    "\n",
    "## Discussion\n",
    "- These models then enable approximation of the docking outcome for unprocessed database entries. Importantly, DL allows the use of simple 2D protein-independent descriptors such as Morgan finger- prints to capture the docking scores. We have demonstrated that such approach can yield a manageably small subset of a database, highly enriched with favorably “dockable” molecular structures.\n",
    "- Moreover, DD appears to enrich final subsets with active ligands, even when only small portions of top ranked molecules are considered. This unexpected result suggests that true binders carry on certain chemical features that are complementary to the binding pocket and that the model is able to capture such features through the QSAR descriptors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23efe419",
   "metadata": {},
   "source": [
    "# Original Protocol summary - *Artificial intelligence–enabled virtual screening of ultra-large chemical libraries with deep docking*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be23c61",
   "metadata": {},
   "source": [
    "- Docking is not just computationally demanding, but also a remarkably wasteful process in which a very small subset of top-scoring compounds is considered for experimental evaluation. Thus, most docked molecules are simply discarded\n",
    "- DD, a technique that iteratively trains deep neural networks (DNNs) with small batches of explicitly docked com- pounds to infer the ranking of the yet-unprocessed remainder of the library\n",
    "\n",
    "## Experimental design\n",
    "### Preparation of chemical libraries\n",
    "- most used ultra-large chemical libraries are ZINC15, ZINC20 and 'make-on-demand' Enamine\n",
    "- Morgan fingerprints with radius 2 and size of 1,024 bits - these extended-connectivity fingerprints represent a machine-readable description of molecules based on a fixed-length binary bit vector encoding the presence or absence of specific substructures \n",
    "\n",
    "### Receptor preparation\n",
    "- Target structures need to be prepared before the docking grids can be generated.\n",
    "- Non-structural water, lipids and solvent molecules are usually removed; the target protein may require structural optimization to repair any missing parts, add hydrogens, compute correct protonation states of residues and energetically relax the structure.\n",
    "\n",
    "### Molecular sample size\n",
    "- Validation, test and initial training sets are randomly sampled from the entire docking library at the first DD pass.\n",
    "- From the second iteration on, the training set is iteratively augmented with random batches of molecules classified as virtual hits in the inference stage of the previous iteration\n",
    "- For library in order of billion compounds, recommended size of validation and test sets in the first iteration as large as possible, ideally comprising 1 million molecules each, and absolutely avoiding using less than 250,000 molecules \n",
    "- The size of the training set, on the other hand, influences mainly model precision, and performances improve with larger training sets (700,000–1,000,000 molecules) and more iterations (8–11).\n",
    "- Validation and test sets are generated only in the first iteration. Because the score threshold used to define virtual hits is decreased at each DD iteration, using small-sized sets can cause generalization issues, especially in the last iteration, in which the number of positive samples in the two sets is very limited (e.g., 0.01%).\n",
    "\n",
    "### Model training and inference\n",
    "- Each iteration step in the DD protocol encompasses model training and inference. To identify virtual hits, the protocol uses binary classifiers in the form of feedforward DNN models (multilayer per- ceptrons) trained on 1,024-bit circular Morgan fingerprints. \n",
    "- Binary ‘positive samples’ in training, validation and test sets are virtual hits with scores above a threshold, corresponding to a predefined top percentage of the docking-ranked molecules in the validation set. The rest of the molecules are labeled as ‘negative samples\n",
    "- After the binary labels are generated, a user-specified number of models with different combi- nations of hyperparameters (number of hidden layers and neurons, dropout frequencies, over- sampling of minority class and class weights) are trained to optimize model test set accuracy by using a grid search strategy\n",
    "- After the training phase is finished for the initial iteration, the optimal binary classifier is used for inference of virtual hit-likeness of the remainder of the molecular library. For the next iterations, training, validation and test sets are augmented with new compounds randomly selected from molecules with predicted virtual hit-likenesses higher than a classification threshold corresponding to a user-defined recall value for validation predictions.\n",
    "- The total number of iterations typically ranges from 4 to 11, and we normally train 24 models at each iteration in the optimization step. For most docking campaigns, these parameters are sufficient to shrink a database of 1–1.5 billion molecules to a few million compounds that could be conventionally docked with regular computa- tional resources. Alternatively, as we mentioned before, the preset recall value could be adjusted for more ‘aggressive’ DD-selection of top-scored compounds.\n",
    "\n",
    "### Applications\n",
    "- The DD protocol can be used in conjunction with any popular docking program. \n",
    "\n",
    "### Comparison with alternative methods\n",
    "- One of the major challenges of modern CADD is a constantly growing need for computational resources required to screen chemical libraries that are exploding in size because of recent advances in automated synthesis and robotics.\n",
    "- OpenEye GigaDocking\n",
    "- Autodock program has been parallelized for Compute Unified Device Architecture (CUDA)45 and deployed on the Summit supercomputer\n",
    "- VirtualFlow\n",
    "- Bender et al. developed a guide for ultra-large docking campaigns\n",
    "- These docking platforms achieved great high-throughput but are extremely resource demanding in comparison to DD.\n",
    "- Conventional docking of ultra-large libraries remains unaffordable for most of the research community\n",
    "- Hence many new machine learning emulation techniques developed\n",
    "- DD is one of the fastest AI-enabled docking platforms and the only method that has been extensively tested on 1B+ libraries. In addition, the DD protocol does not rely on a particular docking program, and thus it is compatible with the emerging large-scale docking methods to improve their high-throughput capabilities.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "- DD is implemented for fast and economical virtual screening and thus provides docking details exclusively for the top-scoring molecules and disregards large fractions of chemical libraries.\n",
    "-  In addition, the quality of DD results entirely depends on the suitability of the docking program to prioritize active molecules from an ultra-large library. Hence, we anticipated that it would be challenging to discover active molecules from DD of a library of a billion molecules if docking performs poorly on the specific target, just like in the case of conventional docking\n",
    "\n",
    "https://pubs.acs.org/doi/10.1021/cn100008c#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa024f66",
   "metadata": {},
   "source": [
    "# Running the protocol - *New automated workflow*\n",
    "\n",
    "<font color=\"red\"> NOTE: Before proceeding, make sure you performed steps from the section **SET-UP**. This section is at the end of the notebook. </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8571ca21",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42ea73f",
   "metadata": {},
   "source": [
    "Packages that were used were put together to *requirements.txt*. You can install it via following. However, it is possible that some of the packages might be redundant.\n",
    "\n",
    "<font color=\"red\"> Note: Tensorflow should be installed separatelly as it is not correctly included in the requirements.txt file. For cuda11, nvidia-tensorflow (https://developer.nvidia.com/blog/accelerating-tensorflow-on-a100-gpus/) is advised.  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c152bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1376a31",
   "metadata": {},
   "source": [
    "Useful ommands to install nvidia tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa5fdfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nvidia-pyindex\n",
    "# pip install nvidia-tensorflow\n",
    "\n",
    "# conda install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5190beb",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5b05a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mb2462/.conda/envs/DD_protocol/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mb2462/.conda/envs/DD_protocol/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mb2462/.conda/envs/DD_protocol/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mb2462/.conda/envs/DD_protocol/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mb2462/.conda/envs/DD_protocol/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mb2462/.conda/envs/DD_protocol/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import rdkit\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib as mpl\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f19595",
   "metadata": {},
   "source": [
    "## CHEMICAL LIBRARY PROCESSING\n",
    "\n",
    "We are using ready-to-screen version of ZINC20 from https://files.docking.org/zinc20-ML/ with SMILES and already calculated Morgan Fingerprints. This library is already prepared and there is no additional need to enumerate stereoisomers, tautomers and protomers and morgan fingerprints are ready. The chemical space contains 1,006,651,037 compounds.\n",
    "\n",
    "Library is split into 100 text files each containing approximatelly 10,000,000 compounds. There is separate folder for smiles and separate folder for corresponding fingerprints.\n",
    "\n",
    "However, if original dataset wants to be used for iteration, or we want to reduce the chemical space based on the physiochemical properties, we need to apply desired filters.\n",
    "\n",
    "To do so, *filter_by_properties.py* script can be used. Below, you can find arguments this script takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "399f3478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: filter_by_properties.py [-h] -file FILE -output_directory\r\n",
      "                               OUTPUT_DIRECTORY [-max_logP MAX_LOGP]\r\n",
      "                               [-max_molWt MAX_MOLWT] [-min_TPSA MIN_TPSA]\r\n",
      "                               [-max_TPSA MAX_TPSA] [-max_HBD MAX_HBD]\r\n",
      "                               [-use_only_molWt USE_ONLY_MOLWT]\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -file FILE            File to process in format SMILES <whitespace>\r\n",
      "                        UNIQUE_ID\r\n",
      "  -output_directory OUTPUT_DIRECTORY\r\n",
      "                        Directory where to save the filtered output\r\n",
      "  -max_logP MAX_LOGP    The upper bound for logP(lipophilicity)\r\n",
      "  -max_molWt MAX_MOLWT  The upper bound for molWt(molecular_weight)\r\n",
      "  -min_TPSA MIN_TPSA    The lower bound for\r\n",
      "                        TPSA(topological_polar_surface_area)\r\n",
      "  -max_TPSA MAX_TPSA    The upper bound for\r\n",
      "                        TPSA(topological_polar_surface_area)\r\n",
      "  -max_HBD MAX_HBD      The upper bound for HBD(hydrogen_bond_donor)\r\n",
      "  -use_only_molWt USE_ONLY_MOLWT\r\n",
      "                        If the filtering should be performed only by\r\n",
      "                        molWt(molecular weight)\r\n"
     ]
    }
   ],
   "source": [
    "!python scripts_3/filter_by_properties.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6565f0f1",
   "metadata": {},
   "source": [
    "As we want to parallelize this process and use it on slurm, *run_filtering_for_multiple_files.sh* (shown below) can be used. This script paralelizes run of *filter_by_properties.py* for each subfile from library. If you want to change parameters to something else than default, pass the parameters so *filter_by_properties.py* has access to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51c2baa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "directory_to_filter=$1\r\n",
      "output_directory=$2\r\n",
      "\r\n",
      "# If you want to use different filtering options, please change it here\r\n",
      "for file in $directory_to_filter/*; \r\n",
      "do sbatch --account=VENDRUSCOLO-SL3-CPU --partition=skylake --nodes=1 --ntasks=1 --cpus-per-task=10 --time=02:00:00 --wrap \"python scripts_3/filter_by_properties.py -file $file -output_directory $output_directory -use_only_molWt True\"\r\n",
      "done\r\n"
     ]
    }
   ],
   "source": [
    "!cat scripts_3/run_filtering_for_multiple_files.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81538c81",
   "metadata": {},
   "source": [
    "Filtering is done using RdKit and enables filtering based on **max logP, max molecular weight, min and max topological polar surface area and max hydrogen bond donor** or just **max molecular weight alone**. There are molecules, for which corresponding SMILES is not in natural form and cannot be read by RdKit. For this, **SMILITE** by https://github.com/rasbt/smilite is used, which tries to retrieve alternative SMILES from ZINC database and use that to determine properties. THe retrieved alternative SMILES might correspond to slightly different version of molecule, but it can be used as a decent approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fdedbc",
   "metadata": {},
   "source": [
    "### Remove duplicates and merge with Morgan fingerprints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ead2c4",
   "metadata": {},
   "source": [
    "Once the filtering is done, *remove_duplicates_and_merge_with_morgan.py* can be used to remove duplicates and filter and merge corresponding Morgan fingerprints with filtered compounds. \n",
    "\n",
    "<font color=\"red\"> Note: </font>If original dataset is used for other reasons, or just one unique molecule per ZINC ID wants to be used, part of script can be uncommented to remove all non-unique ZINC IDs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7683fb",
   "metadata": {},
   "source": [
    "To run this process concurently on batches of files using SLURM, *run_multiple_remove_duplicates_and_merge_with_morgan.sh* can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ae00053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "input_directory_smiles=$1\r\n",
      "input_directory_fingerprints=$2\r\n",
      "output_directory_smiles=$3\r\n",
      "output_directory_fingerprints=$4\r\n",
      "\r\n",
      "sbatch --account=VENDRUSCOLO-SL3-CPU --partition=skylake --nodes=1 --ntasks=1 --cpus-per-task=10 --time=02:00:00 --wrap \"python scripts_3/remove_duplicates_and_merge_with_morgan.py -start_file 0 -end_file 25 -input_directory_smiles $1 -input_directory_fingerprints $2 -output_directory_smiles $3 -output_directory_fingerprints $4\"; \r\n",
      "\r\n",
      "sbatch --account=VENDRUSCOLO-SL3-CPU --partition=skylake --nodes=1 --ntasks=1 --cpus-per-task=10 --time=02:00:00 --wrap \"python scripts_3/remove_duplicates_and_merge_with_morgan.py -start_file 26 -end_file 50 -input_directory_smiles $1 -input_directory_fingerprints $2 -output_directory_smiles $3 -output_directory_fingerprints $4\"; \r\n",
      "\r\n",
      "sbatch --account=VENDRUSCOLO-SL3-CPU --partition=skylake --nodes=1 --ntasks=1 --cpus-per-task=10 --time=02:00:00 --wrap \"python scripts_3/remove_duplicates_and_merge_with_morgan.py -start_file 51 -end_file 75 -input_directory_smiles $1 -input_directory_fingerprints $2 -output_directory_smiles $3 -output_directory_fingerprints $4\";  \r\n",
      "\r\n",
      "sbatch --account=VENDRUSCOLO-SL3-CPU --partition=skylake --nodes=1 --ntasks=1 --cpus-per-task=10 --time=02:00:00 --wrap \"python scripts_3/remove_duplicates_and_merge_with_morgan.py -start_file 76 -end_file 99 -input_directory_smiles $1 -input_directory_fingerprints $2 -output_directory_smiles $3 -output_directory_fingerprints $4\"; "
     ]
    }
   ],
   "source": [
    "!cat scripts_3/run_multiple_remove_duplicates_and_merge_with_morgan.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a4cc99",
   "metadata": {},
   "source": [
    "<font color=\"red\"> Important: </font> Run the following line also to make fingerprint files to be in the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d63df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in path_to_filtered_fingerprints/*.txt; do sed -i 's/?/,/g' $file; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbd19a8",
   "metadata": {},
   "source": [
    "## PRE-PHASE - FILL IN THE log.txt FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f637d14d",
   "metadata": {},
   "source": [
    "Fill in the log file that should ideally be located under project_path/project_name (in this case this is results/abeta) which also coincides with file_path and protein name in our case. Fill in \n",
    "1.  file path, \n",
    "2. protein, \n",
    "3. path to conf file for VINA specific for this protein, \n",
    "4. path to fingerprints folder, \n",
    "5. path to smiles folder, \n",
    "6. name of tool used for docking (with VINA this does not matter)\n",
    "7. number of hyperparameters to test for model, \n",
    "8. size of validation and test sets (the same), \n",
    "9. path to receptor file that will be used during VINA docking, \n",
    "10. path to OBABEL software, \n",
    "11. path to VINA software,\n",
    "11. path to VINA-GPU software. \n",
    "\n",
    "<font color=\"red\"> Important: </font> Each should be on a new line and there should be **NO** trailing white space after the path/name on each line  as this may cause issues when resolving paths. Some paths need to be absolute as some scripts change directory to a specific one from the main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2319a41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mb2462/rds/hpc-work/DD/DD_protocol_data/DD_main/results\r\n",
      "abeta\r\n",
      "/home/mb2462/rds/hpc-work/DD/DD_protocol_data/DD_main/results/abeta/conf.txt\r\n",
      "../library_prepared_filtered_fingerprints                   \r\n",
      "../library_prepared_filtered_smiles\r\n",
      "Vina                                                   \r\n",
      "24                                                   \r\n",
      "700000\r\n",
      "/home/mb2462/rds/hpc-work/DD/DD_protocol_data/DD_main/results/abeta/receptor.pdbqt \r\n",
      "/home/mb2462/test/DD_protocol_data/OPENBABEL/build/bin/obabel\r\n",
      "/home/mb2462/rds/hpc-work/DD/DD_protocol_data/VINA/autodock_vina_1_1_2_linux_x86/bin/vina\r\n",
      "/home/mb2462/rds/hpc-work/DD/DD_protocol_data/VINA_GPU/Vina-GPU"
     ]
    }
   ],
   "source": [
    "!cat results/abeta/logs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d65d49",
   "metadata": {},
   "source": [
    "## (DEPRECATED) PHASE 1 METHOD 1: Original dataset processing\n",
    "\n",
    "### <font color=\"red\"> Note: If used, in iteration 1 you can skip to phase 4 after this. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e3de42",
   "metadata": {},
   "source": [
    "Now that library is prepared, we need to process the original dataset and make sure it is in a suitable form for the next steps of iteration 1. As we have the data docked, phases 2 and 3 can be skipped an we are directly going to phase 4. \n",
    "\n",
    "Before we process, the dataset, we remove duplicates and merge it with the processed library so we make sure all its compounds are also in the library. For this, following function *filter_original_dataset.py* can be used. If no changes in previous steps have been made, the directories and files should be the same and no alterations to the script are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b40e2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\r\n",
      "from os import listdir\r\n",
      "from os.path import isfile, join\r\n",
      "from argparse import ArgumentParser\r\n",
      "\r\n",
      "# IMPORTANT - Make sure the IDs correspond\r\n",
      "\r\n",
      "# Parse arguments\r\n",
      "parser = ArgumentParser()\r\n",
      "parser.add_argument(\"-original_dataset\", required=True,\r\n",
      "                    help=\"Path to directory with files to process\")\r\n",
      "parser.add_argument(\"-directory_to_compare\", required=True,\r\n",
      "                    help=\"Path to directory to compare the file with \")\r\n",
      "args = parser.parse_args()\r\n",
      "\r\n",
      "# If directory_to_compare has \"/\" in its path, remove it for preventing errors in subsequent steps\r\n",
      "if args.directory_to_compare[-1] == \"/\":\r\n",
      "    args.directory_to_compare = args.directory_to_compare[:-1]\r\n",
      "    \r\n",
      "# Load original dataset and drop duplicates\r\n",
      "original_dataset = pd.read_csv(args.original_dataset, delim_whitespace=True, header=None, names=[\"zinc_id\",\"score\"])\r\n",
      "original_dataset= original_dataset.drop_duplicates(subset=\"zinc_id\")\r\n",
      "\r\n",
      "files_in_directory = [f for f in listdir(args.directory_to_compare) if f.endswith('.txt')]\r\n",
      "for file in files_in_directory:\r\n",
      "    df = pd.read_csv( args.directory_to_compare + file, \r\n",
      "                     delim_whitespace=True, header=None, names=[\"smiles\", \"zinc_id\"])\r\n",
      "    dataframes.append(df)\r\n",
      "library = pd.concat(dataframes, axis=0, ignore_index=True)\r\n",
      "\r\n",
      "merged_results= pd.merge(original_dataset, library, on='zinc_id', how='left')\r\n",
      "# Remove compounds(rows) that are not in the library\r\n",
      "merged_results = merged_results[merged_results['smiles'].notna()]\r\n",
      "\r\n",
      "filtered_original_dataset = merged_results.drop(['smiles'], axis=1)\r\n",
      "\r\n",
      "# Save filtered original dataset to file\r\n",
      "filtered_original_dataset.to_csv(args.original_dataset.split('.')[0] + \"_filtered.txt\", \r\n",
      "                           header=None, index=None, sep=' ', mode='a')"
     ]
    }
   ],
   "source": [
    "!cat scripts_3/original_dataset_processing/filter_original_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba4f70e",
   "metadata": {},
   "source": [
    "After filtering, you can check how many remaining compounds you have as following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c83be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wc -l original_dataset_filtered.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56f9ab",
   "metadata": {},
   "source": [
    "Then we create directory *path/protein/iteration_1* to simulate passing of sections phase 2 and phase 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d7daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p results/abeta/iteration_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9714ae",
   "metadata": {},
   "source": [
    "Now we randomly sample the filtered original set and split it to 3 equal sections (train/test/valid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ab8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python scripts_3/original_dataset_processing/split_original_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c29e2b",
   "metadata": {},
   "source": [
    "Run *molecular_file_count_updated.py* as a file that is created by this script will be used in Phase 4. This is usually step used in the random sampling part, however, in iteration 1 we are not random sampling, but rather processing the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9728756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python scripts_1/molecular_file_count_updated.py --project_name abeta --n_iteration 1 --data_directory library_prepared_filtered_fingerprints --tot_process 10 --tot_sampling 1372108\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2caa50",
   "metadata": {},
   "source": [
    "We can now reuse scripts *scripts_1/extracting_morgan.py* and *scripts_1/extracting_smiles.py* from original phase 1 to extract morgan fingerprints and smiles for the selected 3 sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c923b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python scripts_1/extracting_morgan.py -pt abeta -fp results -it 1 -md library_prepared_filtered_fingerprints -t_pos 10\n",
    "# !python scripts_1/extracting_smiles.py -pt abeta -fp results -it 1 -smd library_prepared_filtered_smiles -t_pos 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af928758",
   "metadata": {},
   "source": [
    "After this, we can simulate extraction of labels (score) to prepare the the sets for altered phase 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "508586f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python scripts_3/original_dataset_processing/extract_labels_original_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a693bd",
   "metadata": {},
   "source": [
    "The steps above can be run also in one script (shown below). Memory used by the process should be adjusted though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a69adc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "\r\n",
      "#SBATCH --account VENDRUSCOLO-SL3-CPU\r\n",
      "#SBATCH --partition skylake\r\n",
      "#SBATCH --nodes=1\r\n",
      "#SBATCH --ntasks=1\r\n",
      "#SBATCH --cpus-per-task=10\r\n",
      "#SBATCH --time=01:00:00\r\n",
      "\r\n",
      "file_name=$1\r\n",
      "path_project=$2\r\n",
      "project_name=$3\r\n",
      "path_to_morgan=$4\r\n",
      "path_to_smiles=$5\r\n",
      "\r\n",
      "path_to_store=$path_project/$project_name/iteration_1\r\n",
      "total_sampling=$(wc -l < \"${file_name}\")\r\n",
      "\r\n",
      "echo \"total sampling $total_sampling\"\r\n",
      "\r\n",
      "python scripts_1/molecular_file_count_updated.py --project_name $project_name --n_iteration 1 --data_directory $path_to_morgan --tot_process 10 --tot_sampling $total_sampling\r\n",
      "\r\n",
      "# Filter out original dataset - filter ZINCs that are not in the library. \r\n",
      "# Uncomment if needed. If used, please make sure to update the file_name to the filtered set for the subsequent steps. \r\n",
      "# If used, please adjust the memory needed for the job.\r\n",
      "# python scripts_3/original_dataset_processing/filter_original_dataset.py -original_dataset $file_name -directory_to_compare $path_to_smiles\r\n",
      "\r\n",
      "mkdir -p $path_to_store\r\n",
      "\r\n",
      "# Split dataset to 3 equally size parts\r\n",
      "echo \"splitting $file_name\"\r\n",
      "python scripts_3/original_dataset_processing/split_original_dataset.py -file_to_process $file_name -path_to_store $path_to_store\r\n",
      "\r\n",
      "# Extract Morgan fingerprints and smiles\r\n",
      "echo \"extracting morgan and smiles\"\r\n",
      "python scripts_1/extracting_morgan.py -pt $project_name -fp $path_project -it 1 -md $4 -t_pos 10\r\n",
      "python scripts_1/extracting_smiles.py -pt $project_name -fp $path_project -it 1 -smd $5 -t_pos 10\r\n",
      "\r\n",
      "# Extract labels for original dataset\r\n",
      "echo \"extracting labels\"\r\n",
      "python scripts_3/original_dataset_processing/extract_labels_original_dataset.py -file_name $file_name -path_to_sets $path_to_store"
     ]
    }
   ],
   "source": [
    "!cat phase_1_prepare_original_dataset.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f227af",
   "metadata": {},
   "source": [
    "An example command is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88fa06cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE COMMAND:\n",
    "#!sbatch phase_1_prepare_original_dataset.sh original_dataset.txt results abeta ../library_prepared_filtered_fingerprints ../library_prepared_filtered_smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc65a1",
   "metadata": {},
   "source": [
    "## PHASE 1 METHOD 2: Random sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6db801",
   "metadata": {},
   "source": [
    "This is the method used by the original protocol. Training set size is set in this command, while testing/validation set size is retrieved from the logs file. \n",
    "\n",
    "It can be run using a following commmand as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3aa081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phase_1.sh current_iteration n_cpus_per_node path_project project training_sample_size conda_env\n",
    "#!sbatch --account=VENDRUSCOLO-SL3-CPU --partition=skylake --nodes=1 --ntasks=1 --cpus-per-task=10 --time=01:00:00 phase_1.sh 1 10 results abeta 450000 DD_protocol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cfa9f4",
   "metadata": {},
   "source": [
    "## PHASE 2 - PREPARE LIGANDS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa419ba",
   "metadata": {},
   "source": [
    "To prepare ligands for docking, we are going to download 3D conformations for ligands of ZINC IDs as **SDF** files for compounds that do not have multiple isomers. For the compunds with multiple isomers present (i.e. contain _1/_2/..), we create the 3D conformations using RDKit. All SDF conformations are then converted to **PDBQT** using **OBABEL**.\n",
    "\n",
    "For the 3D conformation generation using RDKit, we use a code by Berenger et al available at https://github.com/UnixJunkie/smi2sdf3d/blob/master/smi2sdf.py has been used. \n",
    "\n",
    "<font color=\"red\"> Note: </font> For some SMILES, the generation might not be successful due to no good conformations produced/available. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0325ea",
   "metadata": {},
   "source": [
    "For this we first split the train/test/valid set to the group of compounds withouth isomers (to have downloaded conformations) and with isomers (to have 3D generated conformations). \n",
    "\n",
    "Then we download 3D conformations for batches of 1000 ligands (similarly we create 3D conformations in batches of 1000), in parallel. For download, we first create download jobs to run downloads in parallel. \n",
    "\n",
    "This step is automated in ***phase_2_vina_mixed_download_and_creation_ligands.sh***.\n",
    "\n",
    "\n",
    "\n",
    "You can run ***phase_2_vina_download_ligands.sh*** using a command similar to following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73479161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRUCTURE: phase_2_vina_mixed_download_and_creation_ligands.sh current_iteration n_cpus_per_node path_project project_name name_cpu_partition account_name\n",
    "#!sbatch --account=VENDRUSCOLO-SL3-CPU --partition=skylake --nodes=1 --ntasks=1 --cpus-per-task=10 --time=02:00:00 phase_2_vina_mixed_download_and_creation_ligands.sh 1 10 results  abeta skylake VENDRUSCOLO-SL3-CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ffab3",
   "metadata": {},
   "source": [
    "Function to create download commands is *create_download_ligand_scripts.py* shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd758608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\r\n",
      "from argparse import ArgumentParser\r\n",
      "\r\n",
      "# Parse the arguments\r\n",
      "parser = ArgumentParser()\r\n",
      "parser.add_argument(\"-file\", required=True,\r\n",
      "                    help=\"File to process\")\r\n",
      "parser.add_argument(\"-path_to_store_scripts\", default=\"\",\r\n",
      "                    help=\"Path where to store scripts\")\r\n",
      "parser.add_argument(\"-path_to_store_ligands\", default=\"\",\r\n",
      "                    help=\"Path where to store the ligands\")\r\n",
      "args = parser.parse_args()\r\n",
      "\r\n",
      "set_to_process = pd.read_csv(args.file, \r\n",
      "                       delim_whitespace=True, header=None, names=[\"ZINC_ID\"])\r\n",
      "# Get only numerical part fo ZINC IDs (remove \"ZINC\")\r\n",
      "set_to_process[\"ZINC_ID\"] = set_to_process[\"ZINC_ID\"].map(lambda x: x[4:])\r\n",
      "\r\n",
      "# Get ZINC IDs\r\n",
      "zinc_ids = list(set_to_process[\"ZINC_ID\"])\r\n",
      "\r\n",
      "# Create chunks of ZINC IDs of size 1000\r\n",
      "chunks = [zinc_ids[x:x+1000] for x in range(0, len(zinc_ids), 1000)]\r\n",
      "\r\n",
      "# Turn chunks into strings of format that will go to the curl command\r\n",
      "chunks_in_string = []\r\n",
      "for chunk in chunks:\r\n",
      "    chunks_in_string.append(\"\\r\\n\".join(chunk))\r\n",
      "    \r\n",
      "# Parts of CURL command to download batches/chunks of SDFs\r\n",
      "curl_first_part = '''curl 'https://zinc20.docking.org/substances/resolved/' \\\r\n",
      "  -H 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9' \\\r\n",
      "  -H 'Accept-Language: sk-SK,sk;q=0.9,cs;q=0.8,en-US;q=0.7,en;q=0.6' \\\r\n",
      "  -H 'Cache-Control: max-age=0' \\\r\n",
      "  -H 'Connection: keep-alive' \\\r\n",
      "  -H 'Content-Type: multipart/form-data; boundary=----WebKitFormBoundary3PFsqZvV99a0nSHH' \\\r\n",
      "  -H 'Cookie: _ga=GA1.2.508312238.1655725625; PHPSESSID=il78hev7mv4gka4ggb47a375i1; _gid=GA1.2.1550734936.1657353286; _gat_gtag_UA_24210718_4=1; session=eyJfZnJlc2giOmZhbHNlLCJjc3JmX3Rva2VuIjoiMDdhMGU2YmEyNWQ1ZjUzYTZhMjA0MGU3N2UxMGJiYjM3YmI4NDI0NyJ9.YssFsg.E9MPTJ2CEcmvY9gOp258TxveVfg' \\\r\n",
      "  -H 'Origin: https://zinc20.docking.org' \\\r\n",
      "  -H 'Referer: https://zinc20.docking.org/substances/home/' \\\r\n",
      "  -H 'Sec-Fetch-Dest: document' \\\r\n",
      "  -H 'Sec-Fetch-Mode: navigate' \\\r\n",
      "  -H 'Sec-Fetch-Site: same-origin' \\\r\n",
      "  -H 'Sec-Fetch-User: ?1' \\\r\n",
      "  -H 'Upgrade-Insecure-Requests: 1' \\\r\n",
      "  -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36' \\\r\n",
      "  -H 'cp-extension-installed: Yes' \\\r\n",
      "  -H 'sec-ch-ua: \".Not/A)Brand\";v=\"99\", \"Google Chrome\";v=\"103\", \"Chromium\";v=\"103\"' \\\r\n",
      "  -H 'sec-ch-ua-mobile: ?0' \\\r\n",
      "  -H 'sec-ch-ua-platform: \"macOS\"' \\\r\n",
      "  --data-raw $'------WebKitFormBoundary3PFsqZvV99a0nSHH\\r\\nContent-Disposition: form-data; name=\"paste\"\\r\\n\\r\\n'''\r\n",
      "\r\n",
      "curl_second_part = '''\\r\\n------WebKitFormBoundary3PFsqZvV99a0nSHH\\r\\nContent-Disposition: form-data; name=\"upload\"; filename=\"\"\\r\\nContent-Type: application/octet-stream\\r\\n\\r\\n\\r\\n------WebKitFormBoundary3PFsqZvV99a0nSHH\\r\\nContent-Disposition: form-data; name=\"identifiers\"\\r\\n\\r\\ny\\r\\n------WebKitFormBoundary3PFsqZvV99a0nSHH\\r\\nContent-Disposition: form-data; name=\"structures\"\\r\\n\\r\\ny\\r\\n------WebKitFormBoundary3PFsqZvV99a0nSHH\\r\\nContent-Disposition: form-data; name=\"names\"\\r\\n\\r\\ny\\r\\n------WebKitFormBoundary3PFsqZvV99a0nSHH\\r\\nContent-Disposition: form-data; name=\"retired\"\\r\\n\\r\\ny\\r\\n------WebKitFormBoundary3PFsqZvV99a0nSHH\\r\\nContent-Disposition: form-data; name=\"charges\"\\r\\n\\r\\ny\\r\\n------WebKitFormBoundary3PFsqZvV99a0nSHH\\r\\nContent-Disposition: form-data; name=\"scaffolds\"\\r\\n\\r\\ny\\r\\n------WebKitFormBoundary3PFsqZvV99a0nSHH\\r\\nContent-Disposition: form-data; name=\"multiple\"\\r\\n\\r\\ny\\r\\n------WebKitFormBoundary3PFsqZvV99a0nSHH\\r\\nContent-Disposition: form-data; name=\"output_format\"\\r\\n\\r\\nsdf\\r\\n------WebKitFormBoundary3PFsqZvV99a0nSHH--\\r\\n' \\\r\n",
      "  --compressed'''\r\n",
      "\r\n",
      "# Build individual bash script for each batch/chunk.\r\n",
      "for index,chunk in enumerate(chunks_in_string):\r\n",
      "    output_name = args.path_to_store_ligands + \"/chunk_\" + str(index)+ \".sdf\"\r\n",
      "    curl_command = curl_first_part + chunk + curl_second_part + \" > \" + output_name + \"\\n\"\r\n",
      "    number_of_lines_in_file_command = 'x=$(wc -l < ' + output_name + ' )\\n'\r\n",
      "    # Retry command is used if the request has failed (output file has <1000 lines). \r\n",
      "    retry_command = 'if [ $x -lt 1000 ];\\n then\\n ' + 'echo \"Retrying download as request failed\"\\n' + curl_command + ' fi\\n'\r\n",
      "    with open (args.path_to_store_scripts + '/download_chunk_' + str(index) + '.sh', 'w') as script:\r\n",
      "            script.write('#! /bin/bash\\n')\r\n",
      "            script.write('module load curl-7.63.0-intel-17.0.4-lxwgw2f\\n') # load newer version of CURL (relevant on CSD3 only)\r\n",
      "            script.write(curl_command)\r\n",
      "            # If request fail, retry up to 5 times. This is done this way and not with --retry curl option so we do not \r\n",
      "            # have faulty html snippet in the output sdf file.\r\n",
      "            script.write(number_of_lines_in_file_command)\r\n",
      "            script.write(retry_command)\r\n",
      "            script.write(number_of_lines_in_file_command)\r\n",
      "            script.write(retry_command)\r\n",
      "            script.write(number_of_lines_in_file_command)\r\n",
      "            script.write(retry_command)\r\n",
      "            script.write(number_of_lines_in_file_command)\r\n",
      "            script.write(retry_command)\r\n",
      "            script.write(number_of_lines_in_file_command)\r\n",
      "            script.write(retry_command)\r\n"
     ]
    }
   ],
   "source": [
    "!cat scripts_3/create_download_ligand_scripts.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbcdd4e",
   "metadata": {},
   "source": [
    "<font color=\"red\"> Important: </font> Script utilizes new version of curl and hence module *curl-7.63.0-intel-17.0.4-lxwgw2f* needs to be loaded when using CSD3. As the request may fail sometimes, command is designed to be retried a few times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc8b2c4",
   "metadata": {},
   "source": [
    "<font color=\"red\"> Important: </font> As the request may fail even when tried a few times, a separate script can be used to download the batches for which the requests have failed. This should not happen for many batches. The script can be rerun number of times until all batches are downloaded. \n",
    "\n",
    "This scripts retries downloads for scripts that did not complete successfully or did not finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dd96321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "\r\n",
      "#SBATCH --account VENDRUSCOLO-SL3-CPU\r\n",
      "#SBATCH --partition skylake\r\n",
      "#SBATCH --nodes=1\r\n",
      "#SBATCH --ntasks=1\r\n",
      "#SBATCH --cpus-per-task=10\r\n",
      "#SBATCH --time=10:00:00\r\n",
      "\r\n",
      "current_iteration=$1\r\n",
      "path_project=$2\r\n",
      "project_name=$3\r\n",
      "\r\n",
      "# Get paths\r\n",
      "file_path=`sed -n '1p' $path_project/$project_name/logs.txt`\r\n",
      "protein=`sed -n '2p' $path_project/$project_name/logs.txt`\r\n",
      "\r\n",
      "# Go to directory with the current iteration\r\n",
      "cd $file_path/$protein/iteration_${current_iteration}\r\n",
      "\r\n",
      "pdbqt_directory=\"pdbqt\"\r\n",
      "\r\n",
      "# For each batch file that has not been downloaded due to request failure, run the download again.\r\n",
      "echo \"retry based on number of lines\"\r\n",
      "for d in ${pdbqt_directory}/*_download;\r\n",
      "do\r\n",
      "tmp=\"$d\"\r\n",
      "directory_set_name_full=\"${tmp##*/}\"\r\n",
      "set_type=\"${directory_set_name_full%_*}\" # train/test/validation\r\n",
      "echo $set_type\r\n",
      "   for f in $d/*.sdf\r\n",
      "   do\r\n",
      "       x=$(wc -l < \"$f\")\r\n",
      "       if [ $x -lt 1000 ];\r\n",
      "       then\r\n",
      "           tmp=\"$f\"\r\n",
      "           full_filename=\"${tmp##*/}\"\r\n",
      "           filename=\"${full_filename%.*}\"\r\n",
      "           script_name=${set_type}_set_scripts/download_${filename}.sh\r\n",
      "           echo \"Retrying script ${script_name}\"\r\n",
      "        #    chmod u+x $script_name\r\n",
      "        #    ./$script_name\r\n",
      "       fi\r\n",
      "   done\r\n",
      "done\r\n",
      "\r\n",
      "# For each batch file that has less compounds downloaded than 1000, repeat the download\r\n",
      "echo \"retry based on number of compounds\"\r\n",
      "for d in ${pdbqt_directory}/*_download;\r\n",
      "do\r\n",
      "tmp=\"$d\"\r\n",
      "directory_set_name_full=\"${tmp##*/}\"\r\n",
      "set_type=\"${directory_set_name_full%_*}\" # train/test/validation\r\n",
      "echo $set_type\r\n",
      "   for f in $d/*.sdf\r\n",
      "   do\r\n",
      "       x=$(grep -wc \"\\$\\$\\$\\$\" < \"$f\")\r\n",
      "       if [ $x -lt 1000 ];\r\n",
      "       then\r\n",
      "           tmp=\"$f\"\r\n",
      "           full_filename=\"${tmp##*/}\"\r\n",
      "           filename=\"${full_filename%.*}\"\r\n",
      "           script_name=${set_type}_set_scripts/download_${filename}.sh\r\n",
      "           echo \"Retrying script ${script_name}\"\r\n",
      "        #    chmod u+x $script_name\r\n",
      "        #    ./$script_name\r\n",
      "       fi\r\n",
      "   done\r\n",
      "done\r\n"
     ]
    }
   ],
   "source": [
    "!cat phase_2_vina_retry_downloads.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6abac4",
   "metadata": {},
   "source": [
    "Example run of the retry function is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ba4d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sbatch phase_2_vina_retry_downloads.sh 1 results abeta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9f75db",
   "metadata": {},
   "source": [
    "### FINAL LIGANDS PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519292c9",
   "metadata": {},
   "source": [
    "Following either alternative or RDKit method, we end up with SDF files containing confromations of batches of ligands. We then have to split ligands to separate files and then convert them to PDBQT format.\n",
    "\n",
    "This can be done using *phase_2_vina_prepare_ligands.sh* function and an example comand is shown below.\n",
    "\n",
    "<font color=\"red\"> CSD3 Note Only: </font> Sometimes you might get libboost_iostreams.so.1.66.0 error. For this you should *module load gcc* and *module load boost-1.66.0-gcc-5.4.0-sdffwvs*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92c46f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRUCTURE phase_2_vina_prepare_ligands.sh current_iteration path_project project_name name_cpu_partition account_name\n",
    "#!sbatch phase_2_vina_prepare_ligands.sh 1 results abeta skylake VENDRUSCOLO-SL3-CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c8b411",
   "metadata": {},
   "source": [
    "### <font color=\"red\"> Important: </font> CSD3 *rds* storage has a limit on number of files stored - 1 million. When spliting SDFs to separate files, this can cause issues as there will be not enough space if preparing more than 1 million ligands. An option would be to do preparation and docking one by one for each set (train/test/valid). Alternative is to create files and ZIP them to have only *one* file.\n",
    "\n",
    "### <font color=\"red\"> Important: </font> This step can be also outsourced to *Marcopolo*, downloads/RDKit generation as well as ligand preparation can be run there. The files can then be moved and docked on CSD3. This allows also to utilize more resource power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391e194f",
   "metadata": {},
   "source": [
    "## PHASE 3 - Docking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9ff1e4",
   "metadata": {},
   "source": [
    "In this step, we dock the molecules from the sets that need to be docked using **VINA** or **VINA-GPU** based on the user's preference. We dock each batch within each set separately and output the docking results for given batch as a concatenated txt file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a287ce73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "#SBATCH --nodes=1\r\n",
      "#SBATCH --ntasks=1\r\n",
      "#SBATCH --gres=gpu:1 # COMMENT OUT WHEN USING CLASSIC VINA\r\n",
      "#SBATCH --cpus-per-task=1\r\n",
      "#SBATCH --time=02:15:00\r\n",
      "\r\n",
      "# PARAMETERS\r\n",
      "batch_directory_to_dock=$1 # THIS SHOULD BE A FULL PATH IF USING VINA GPU\r\n",
      "receptor=$2 # THIS SHOULD BE A FULL PATH IF USING VINA GPU\r\n",
      "configuration_file=$3 # THIS SHOULD BE A FULL PATH IF USING VINA GPU\r\n",
      "path_to_output_directory=$4 # THIS SHOULD BE A FULL PATH IF USING VINA GPU\r\n",
      "vina_path=$5\r\n",
      "using_vina_gpu=$6\r\n",
      "\r\n",
      "tmp=\"${batch_directory_to_dock}\"\r\n",
      "batch=\"${tmp##*/}\"\r\n",
      "\r\n",
      "# Prepare output file paths\r\n",
      "output_file_txt=$path_to_output_directory/docking_vina_gpu_$batch.txt\r\n",
      "output_file_pdbqt=$path_to_output_directory/docking_vina_gpu_${batch}_dummy.pdbqt\r\n",
      "\r\n",
      "echo $batch_directory_to_dock\r\n",
      "\r\n",
      "# Use Vina or Vina-GPU based on the user's choice.\r\n",
      "if [ \"$using_vina_gpu\" = true ] ; then\r\n",
      "    echo \"Using VINA-GPU\"\r\n",
      "    ulimit -s 8192\r\n",
      "    # For each file in the batch directory, run VINA docking. Output of all dockings along with corresponding ZINC IDs is stored\r\n",
      "    # to 1 file.\r\n",
      "    cd $vina_path\r\n",
      "    for file in $batch_directory_to_dock/*.pdbqt; do\r\n",
      "        echo $file\r\n",
      "        tmp=\"$file\"\r\n",
      "        full_filename=\"${tmp##*/}\"\r\n",
      "        zinc_id=\"${full_filename%.*}\"\r\n",
      "        echo -e \"\\n\" >> $output_file_txt\r\n",
      "        echo $zinc_id >> $output_file_txt\r\n",
      "        ./Vina-GPU --receptor $receptor --config $configuration_file --thread 8000 --search_depth 10 --ligand $file >> $output_file_txt\r\n",
      "        echo \"@@@@\" >> $output_file_txt\r\n",
      "        rm ${batch_directory_to_dock}/${zinc_id}_out.pdbqt\r\n",
      "    done\r\n",
      "else \r\n",
      "    echo \"Using AutoDock Vina\"\r\n",
      "    # For each file in the batch directory, run VINA docking. Output of all dockings along with corresponding ZINC IDs is stored\r\n",
      "    # to 1 file.\r\n",
      "    for file in $batch_directory_to_dock/*.pdbqt; do\r\n",
      "        tmp=\"$file\"\r\n",
      "        full_filename=\"${tmp##*/}\"\r\n",
      "        zinc_id=\"${full_filename%.*}\"\r\n",
      "        echo -e \"\\n\" >> $output_file_txt\r\n",
      "        echo $zinc_id >> $output_file_txt\r\n",
      "        $vina_path --receptor $receptor --config $configuration_file --cpu 10 --exhaustiveness 20 --ligand $file --out $output_file_pdbqt >> $output_file_txt\r\n",
      "        echo \"@@@@\" >> $output_file_txt\r\n",
      "    done;\r\n",
      "fi\r\n"
     ]
    }
   ],
   "source": [
    "!cat scripts_3/run_batch_docking.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae986b",
   "metadata": {},
   "source": [
    "<font color=\"red\"> Note: </font> For AutoDock Vina, comment out *#SBATCH --gres=gpu:1 to spare the resources*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd0832d",
   "metadata": {},
   "source": [
    "Example command to run of docking for Vina and VINA-GPU is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c653958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRUCTURE phase_3_vina.sh current_iteration path_project project_name use_vina_gpu account_name partition\n",
    "\n",
    "# For VINA-GPU\n",
    "#!sbatch phase_3_vina.sh 1 results abeta true VENDRUSCOLO-SL3-GPU ampere\n",
    "\n",
    "# For AutoDock Vina\n",
    "#!sbatch phase_3_vina.sh 1 results abeta false VENDRUSCOLO-SL3-CPU skylake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e84dd5",
   "metadata": {},
   "source": [
    "### PHASE 3 - post processing\n",
    "\n",
    "Now that we have docked data, we need to extract labels. We also have to correct the smiles and morgan fingerprints for the downloaded compounds so the model training in the next step is as accurate as possible. \n",
    "\n",
    "For this we can run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aed010a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUCTURE phase_3_vina_post_processing.sh path_to_iteration\n",
    "#!sbatch phase_3_vina_post_processing.sh results/abeta/iteration_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912053e9",
   "metadata": {},
   "source": [
    "## PHASE 4 - model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3583639a",
   "metadata": {},
   "source": [
    "<font color=\"red\"> Note (relevant mostly to CSD3): </font> If you did not set up nvidia-tensorflow for cuda11 directly to your conda environment, I would suggest setting up a separate environment with nvidia-tensorflow that could be used for phase 4 and phase 5.\n",
    "\n",
    "This step is almost the same as in the original pipeline, with code a bit commented and catch statements added. Extraction of labels is excluded here as it is included in the post processing step of phase 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5403882d",
   "metadata": {},
   "source": [
    "We can regularly run phase 4 (that is slightly adjusted) with command similar to the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4db77caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRUCTURE: phase_4_vina.sh current_iteration number_of_processors_available project_path project_name gpu_partition_name desired_final_number_of_iterations percent_first_mols_hits percent_last_mols_hits recall_value max_wall_time conda_environment_name account_name\n",
    "#!sbatch phase_4_vina.sh 1 3 results abeta ampere 11 1 0.01 0.9 00-12:00 DD_protocol_tensor VENDRUSCOLO-SL3-GPU\n",
    "#!sbatch phase_4_vina.sh 1 3 results abeta ampere 5 1 0.01 0.9 00-12:00 DD_protocol_tensor VENDRUSCOLO-SL3-GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd35288",
   "metadata": {},
   "source": [
    "## PHASE 5 - Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba754b0",
   "metadata": {},
   "source": [
    "<font color=\"red\"> Note (relevant mostly to CSD3): </font> If you did not set up nvidia-tensorflow for cuda11 directly to your conda environment, I would suggest setting up a separate environment with nvidia-tensorflow that could be used for phase4 and phase 5.\n",
    "\n",
    "Inference can be used with no major change from the original pipeline (just added commentary and catch statements). There is an update in *simple_job_predictions.py* that also takes a **full morgan fingerprints directory path** as there were issues if the path was not full. An example command is as following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0909bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRUCTURE:  phase_5_vina.sh current_iteration path_to_project project_name recall_value gpu_partition env account_name \n",
    "# !sbatch phase_5_vina.sh 1 results abeta 0.9 ampere DD_protocol_tensor VENDRUSCOLO-SL3-GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5d33a",
   "metadata": {},
   "source": [
    "This command first evaluates the best performing model (*hyperparameter_result_evaluation.py*) and then infers scores on whole library as separate job on each library file.\n",
    "\n",
    "<font color=\"red\"> Important: </font>  For each iteration, check AUC, precision and recall if they are as expected. As advised in the original protocol,\n",
    "\n",
    "- Precision should be at least 2.25% (0.0225)\n",
    "- Recall should not differ by more than 0.015. \n",
    "- *Total Left Testing* in the *best_model_stats* and number of molecules in *morgan_1024_predictions* should not be too much appart (advised max 10%).\n",
    "\n",
    "If these are not, follow advise in the original protocol - regenerate test/valid set, use bigger size of test/valid set etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed573b9",
   "metadata": {},
   "source": [
    "<font color=\"red\"> Note: </font>If there are issues with Keras and you are getting error ***AttributeError: 'str' object has no attribute 'decode'***, run the following command to resolve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66232214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install 'h5py==2.10.0' --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c24fd",
   "metadata": {},
   "source": [
    "## FINAL PHASE - extraction\n",
    "To extract relevant number of molecules (or requesting all molecules with 'all_mol' instead of an integer) we can use command shown below. We decided to extract 3 million molecules that will be further processed down the line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb850a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRUCTURE: sbatch --cpus-per-task no_of_cpus utilities/final_extraction.sh path_to_smiles_directory path_to_last_predicted_hits num_of_cores num_of_molecules_to_extract(or 'all_mol') conda_environment\n",
    "# sbatch --cpus-per-task 10 utilities/final_extraction.sh /home/mb2462/rds/hpc-work/DD/DD_protocol_data/library_ready_filtered_with_isomers_smiles /home/mb2462/rds/hpc-work/DD/DD_protocol_data/DD_main_clean/results/abeta/iteration_5/morgan_1024_predictions 10 3000000 DD_protocol "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2560d930",
   "metadata": {},
   "source": [
    "## CLUSTERING\n",
    "\n",
    "<font color=\"red\"> Note: Full clustering and downstream analysis can be found in **clustering/Clustering_and_downstream_analysis.ipynb** notebook </font>\n",
    "\n",
    "As we have a large number of molecules to cluster (3 million), we cannot use a traditional Butina clustering with RDKit. Following   https://www.macinchem.org/reviews/clustering/clustering.php we can cluster molecules with Chemfp, which does allow clustering larger libraries. We can use 1.x developer line, which is non-commercial. Important to note is that Chemfp 1.x is **not compatibile with Python 3**, hence we have to create a separate environment that will run the code in **Python 2.7**. All steps to create environment, install combatibile RDKit (versions before 2019) and finally chemfp iis shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "521e9dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create -y -n DD_protocol_py27 python=2.7\n",
    "# conda activate DD_protocol_py27\n",
    "# conda install -c rdkit rdkit=2018.09.1\n",
    "# pip install chemfp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bd525ed",
   "metadata": {},
   "source": [
    "Now, to create a compatibile fingerprints from smiles for the molecules we want to cluster we can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107a037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbatch --account=VENDRUSCOLO-SL3-CPU --partition=skylake --nodes=1 --ntasks=1 --cpus-per-task=10 --time=02:00:00 --wrap=\"rdkit2fps extracted_smiles.smi --morgan --radius 2 --useChirality 1 > extracted_smiles.fps\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26903e17",
   "metadata": {},
   "source": [
    "And to get the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb14327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbatch --account=VENDRUSCOLO-SL3-CPU --partition=skylake-himem --nodes=1 --ntasks=1 --cpus-per-task=10 --time=10:30:00 --wrap=\"python ../scripts_3/taylor_butina.py --profile --threshold 0.78 extracted_smiles.fps -o extracted_smiles_clusters.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e483aa",
   "metadata": {},
   "source": [
    "## Possible issues and hacks that help with coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5622198f",
   "metadata": {},
   "source": [
    "1. When running GPU processes on ampere, and then trying to run CPU processes the same login node can give error such as ***/lib64/libc.so.6: version GLIBC_2.27' not found (required by /usr/local/software/slurm/slurm-20.11.9-rhel8/lib/slurm/libslurmfull.so)***. In that case, try switching to a different login node, maybe CPU-exlusive ones such as user-id@login-e-9.hpc.cam.ac.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68c814b",
   "metadata": {},
   "source": [
    "2. When trying to find out number of lines in a file (in our case lines correspond to compounds), you can do this by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c31c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wc -l filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd7d991",
   "metadata": {},
   "source": [
    "3. Remove specific type file in all subdirectories of directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ce7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!for subd in */; do cd $subd; rm *.sdf; cd ..; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b1a37",
   "metadata": {},
   "source": [
    "4. Find number of files in each subdirectory of directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31efcdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!for subd in */;do cd $subd; ls | wc -l; cd ..;done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c3da28",
   "metadata": {},
   "source": [
    "5.  You can run zip and unzip as following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afcc1367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZIP DIRECTORY\n",
    "#!sbatch  --account=VENDRUSCOLO-SL3-CPU --partition=skylake --nodes=1 --ntasks=1 --cpus-per-task=10 --time=03:00:00 --wrap \"zip -r --quiet test_iteration_1.zip test\"\n",
    "\n",
    "# UNZIP DIRECTORY\n",
    "#!sbatch  --account=VENDRUSCOLO-SL3-CPU --partition=skylake --nodes=1 --ntasks=1 --cpus-per-task=10 --time=03:00:00 --wrap \"unzip test_iteration_1.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a15b544",
   "metadata": {},
   "source": [
    "6. Count occurences of word in a file (for all files in the directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db9ae490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1\n",
    "# for f in *; do grep -o 'ZINC'  $f | wc -l; done;\n",
    "# OPTION 2 - for characters that need escaping, do \\character ( slash followed by character)\n",
    "# for f in *; do grep -c '\\$\\$\\$\\$' $f; done;  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e44063",
   "metadata": {},
   "source": [
    "# Set-up to run this notebook from CSD3\n",
    "\n",
    "### This pipeline is meant to be run on Cambridge university cluster CSD3, with possible employment of additional CPU cluster to run CPU computations somewhere else, such as *Marcopolo* cluster. However, this can be adjusted to any cluster with sufficient  <font color=\"red\">  *space* </font> and  <font color=\"red\">  *resources* </font>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2905a47",
   "metadata": {},
   "source": [
    "### Technical Requirements:\n",
    "1. *Enough CPU and GPU resources* - Ideally at least 200 CPU cores and 50 GPU cores, preferably with exclusive access, if not at least with sufficiently short queues.\n",
    "2. *Space* - library itself is around 260GB. Hence reserved space of at least 500GB is preferred\n",
    "3. *File number limit* - running Vina requires separate file per ligand. Hence if 1,000,000 ligands are being docked, the disk should allow that and not have low limit on number of files.\n",
    "\n",
    "\n",
    "#### CSD3 Note: Due to space limits, the big project files (such as datasets) should be stored in the *rds* space on the CSD3. The whole project can be there too, however, there are number of file limits (1 million files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089da9e1",
   "metadata": {},
   "source": [
    "#### 1. Register for access for CSD3 https://www.hpc.cam.ac.uk/rcs-application . If Marcopolo will be used as well, register for that too (ask someone on the team for access).\n",
    "For CSD3: The default for the team is SL3 option (lower service level) which means access to 200,000 CPU core hours and 3000 GPU hours per quarter per PI. SL3 is also limited to per-job, per-user GPU limits 32 GPUs and additional job runtime limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07950051",
   "metadata": {},
   "source": [
    "#### 2. Follow user guide (https://docs.hpc.cam.ac.uk/hpc/user-guide/quickstart.html) to familiarize yourself with CSD3, logging in, get hand of usage of SLURM, job submits (even though the code should include SLURM commands) and file transfers.\n",
    "\n",
    "<font color=\"red\">Note: </font> *CSD3 **does not** seem to allow scripts adjusting memory( --m= desired_memory).*\n",
    "\n",
    "Once you are more familiar with SLURM and job submits, you can try to navigate to the directory *test_print_numbers* and look at scripts that schedule run of *test_print_numbers.py* to see how the process works.\n",
    "\n",
    "The script is run the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf47f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sbatch run_test_print_multiple_files.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae0221",
   "metadata": {},
   "source": [
    "To see the state of the jobs, see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b4e25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       JobID      User    Account    JobName  Partition                 End ExitCode      State  CompHrs\r\n",
      "------------ --------- ---------- ---------- ---------- ------------------- -------- ---------- --------\r\n"
     ]
    }
   ],
   "source": [
    "# FOR CPU JOBS:\n",
    "!gstatement -p vendruscolo-sl3-cpu\n",
    "# FOR GPU JOBS:\n",
    "!gstatement -p vendruscolo-sl3-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a47fd73",
   "metadata": {},
   "source": [
    "Simultaneously, you can find an output of the job in *slurm-jobID.out* file, e.g. *slurm-63016496.out*. Status of a job can be checked also with the following command (if the job is still active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f138c036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slurm_load_jobs error: Invalid job id specified\r\n"
     ]
    }
   ],
   "source": [
    "!scontrol show job 63016496"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6502c2df",
   "metadata": {},
   "source": [
    "To see quota and credits available, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "385200a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem/Project    GB        quota     limit          grace           files    quota    limit   grace User/Grp/Proj\n",
      "/home                 25.7       50.0      55.0                     -    ------- No File Quotas  ------- U:mb2462\n",
      "/rds-d7              633.7     1099.5    1209.5                     -   190192  1048576  1048576       - P:44042\n"
     ]
    }
   ],
   "source": [
    "!quota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "172c17b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User           Usage |        Account     Usage | Account Limit Available (hours)\r\n",
      "---------- --------- + -------------- --------- + ------------- ---------\r\n",
      "mb2462         3,622 | VENDRUSCOLO-SL3-CPU   182,391 |       377,975   195,584\r\n",
      "mb2462         2,071 | VENDRUSCOLO-SL3-GPU    20,410 |        23,358     2,948\r\n"
     ]
    }
   ],
   "source": [
    "!mybalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8341",
   "metadata": {},
   "source": [
    "#### 3. Copy the directory with the code (including this jupyter notebook) to the system using the following <font color=\"red\"> *scp* </font>command from your local console (similarly, copy any additional files you need).   \n",
    "\n",
    "##### Note: Ideally, keep big files in the <font color=\"red\"> *rds*</font> disk space, as this has 1TB limit. However, it does also have limit on number of files - 1,000,000.\n",
    "\n",
    "<font color=\"red\">For later: </font> Copying from system to the local computer works similarly just with switched order. Alternative can be using **rsync** command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f6622",
   "metadata": {},
   "outputs": [],
   "source": [
    "scp DD_code.zip user@login-icelake.hpc.cam.ac.uk:[destination]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55527ee5",
   "metadata": {},
   "source": [
    "#### 4. Download required ZINC20 files using <font color=\"red\"> *wget* </font> from https://files.docking.org/zinc20-ML/. Download smile files as well as fingerprints. Make sure, that all smile files (*smiles_all_{no}.txt*) are in directory together *LIBRARY_PREPARED* and all fingerprints  (*smiles_all_{no}.txt*)  are in directory together *FINGERPRINTS*  to prevent confusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0d20a",
   "metadata": {},
   "source": [
    "#### 5. Set up a conda environment following https://docs.hpc.cam.ac.uk/hpc/software-tools/python.html#using-anaconda-python\n",
    "\n",
    "<font color=\"red\">Note: </font> Do not forget to activate the environment before every usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997b6b85",
   "metadata": {},
   "source": [
    "#### 6. Install VINA from https://vina.scripps.edu/downloads/ . This is version 1.1.2. If higher version is required or other program is required, change this accordingly.\n",
    "\n",
    "<font color=\"red\">Note: </font> VINA does take quite long to dock molecules. If GPU power is available, try to set up and use **VINA GPU** (https://github.com/DeltaGroupNJUPT/Vina-GPU) instead, following steps they mention for Linux set-up. You can install boost and cuda or load them as modules (CUDA is automatically loaded on q nodes). To find paths they are asking for, look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd61229e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.7.4 ('base')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# !module show module_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3797cf0a",
   "metadata": {},
   "source": [
    "Other two variables, GPU_PLATFORM and OPENCL_VERSION can be left as they are. To achieve good precision, try setting *thread=8000* and *search_depth=10*, or adjust these values based on your need. The automatic code uses *thread=8000* and *search_depth=10*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8079694",
   "metadata": {},
   "source": [
    "#### 7. Install Open Babel from https://github.com/openbabel/openbabel/releases/tag/openbabel-3-1-1. On CSD3 (CentOS), this has to be built from source, follow instructions for this online at https://openbabel.org/docs/dev/Installation/install.html.\n",
    "\n",
    "<font color=\"red\">Note: </font> On *login-e-{no}* nodes, there could be issues with libboost. Fix these by loading appropriate modules -  *module load gcc* is required as well as one of the boost gcc modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faadcf0",
   "metadata": {},
   "source": [
    "#### 8. (Alternative available below) Set up a jupyter notebook and forwarding as per user guide (https://docs.hpc.cam.ac.uk/hpc/software-packages/jupyter.html#running-jupyter)\n",
    "\n",
    "<font color=\"red\">Note: </font> As there is a typo in the docs, from local machine, use the following statement (with an appropriate login node, here *login-e-1* is used) instead"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d25f9d64",
   "metadata": {},
   "source": [
    "ssh -L 8081:127.0.0.1:8081 -fN username@login-e-1.hpc.cam.ac.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d9e5b",
   "metadata": {},
   "source": [
    "#### 9. (Alternative available below) Run the jupyter notebook as per guide, open this notebook and follow up on the previous section *Protocol summary and walk-through*\n",
    "\n",
    "<font color=\"red\">Note: </font> It can happen that when the connection is lost or is ended in an incorrect way, the port remains in use and hence you cannot re-establish new connection on a port. For that, on the local machine, run"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aec77c1e",
   "metadata": {},
   "source": [
    "sudo lsof -i:8081\n",
    "kill $PID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d18de",
   "metadata": {},
   "source": [
    "#### 8-9. (Alternative) Open this repository and work on cluster via Visual Studio Code and follow up on the previous section *Protocol summary and walk-through*\n",
    "\n",
    "Protocol and this notebook is available to view through it as well. Follow steps described on https://code.visualstudio.com/docs/remote/ssh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DD_protocol_py27",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.18 |Anaconda, Inc.| (default, Jun  4 2021, 14:47:46) \n[GCC 7.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "23f87cd39c146924c782f3c847cee3a4cce40699555e80c8a92a3c369e622b07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
